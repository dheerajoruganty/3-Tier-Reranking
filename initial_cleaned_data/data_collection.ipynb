{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Necessary Packages \n",
    "\n",
    "Before we can get the data, we need to install some packages to handle the download process. In particular, we are going to install one main package:\n",
    "\n",
    "*   ir_datasets (https://github.com/allenai/ir_datasets): A python package that provides a common interface to many IR ad-hoc ranking benchmarks, training datasets, etc. We can use this to download the raw event streams and information needs for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/allenai/ir_datasets.git@crisisfacts\n",
      "  Cloning https://github.com/allenai/ir_datasets.git (to revision crisisfacts) to /private/var/folders/kd/mjydk2vd0pg_0q8h24fjc1l40000gn/T/pip-req-build-3tsj0ktn\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/allenai/ir_datasets.git /private/var/folders/kd/mjydk2vd0pg_0q8h24fjc1l40000gn/T/pip-req-build-3tsj0ktn\n",
      "  Running command git checkout -b crisisfacts --track origin/crisisfacts\n",
      "  Switched to a new branch 'crisisfacts'\n",
      "  branch 'crisisfacts' set up to track 'origin/crisisfacts'.\n",
      "  Resolved https://github.com/allenai/ir_datasets.git to commit 3d657a7b9f159382776763ee093fc6ef2d7c6581\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (4.12.2)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (4.9.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (6.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (4.66.6)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.1 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (4.3.2)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (3.3.0)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (0.1.12)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from ir-datasets==0.5.2) (0.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from beautifulsoup4>=4.4.1->ir-datasets==0.5.2) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from requests>=2.22.0->ir-datasets==0.5.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from requests>=2.22.0->ir-datasets==0.5.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from requests>=2.22.0->ir-datasets==0.5.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from requests>=2.22.0->ir-datasets==0.5.2) (2024.8.30)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /Users/pranavpatil/anaconda3/lib/python3.11/site-packages (from trec-car-tools>=2.5.4->ir-datasets==0.5.2) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/allenai/ir_datasets.git@crisisfacts # install ir_datasets (crisisfacts branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the CrisisFACTs Dataset\n",
    "\n",
    "The CrisisFACTs dataset is divided into events, representing real-world crises. Each event is given an identifier, e.g. 'CrisisFACTS-001' is the Lilac Wildfire from 2017. We sometimes refer to the event number or 'eventNo', this is the last three digits of the event identifier, e.g. '001'. There are 8 events for CrisisFACTs 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event numbers as a list\n",
    "eventNoList = [\n",
    "    \"001\", # Lilac Wildfire 2017\n",
    "    \"002\", # Cranston Wildfire 2018\n",
    "    \"003\", # Holy Wildfire 2018\n",
    "    \"004\", # Hurricane Florence 2018\n",
    "    \"005\", # 2018 Maryland Flood\n",
    "    \"006\", # Saddleridge Wildfire 2019\n",
    "    \"007\", # Hurricane Laura 2020\n",
    "    \"008\", # Hurricane Sally 2020\n",
    "    \"009\", # Beirut Explosion, 2020\n",
    "    \"010\", # Houston Explosion, 2020\n",
    "    \"011\", # Rutherford TN Floods, 2020\n",
    "    \"012\", # TN Derecho, 2020\n",
    "    \"013\", # Edenville Dam Fail, 2020\n",
    "    \"014\", # Hurricane Dorian, 2019\n",
    "    \"015\", # Kincade Wildfire, 2019\n",
    "    \"016\", # Easter Tornado Outbreak, 2020\n",
    "    \"017\", # Tornado Outbreak, 2020 Apr\n",
    "    \"018\", # Tornado Outbreak, 2020 March\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each event has a duration, i.e. it lasts for a number of days. In the CrisisFACTs track, you need to produce a timeline summary for each day for a set of events. You can get the list of days for an event as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-23\n",
      "2020-01-24\n",
      "2020-01-25\n",
      "2020-01-26\n",
      "2020-01-27\n",
      "2020-01-28\n",
      "2020-01-29\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Gets the list of days for a specified event number, e.g. '001'\n",
    "def getDaysForEventNo(eventNo):\n",
    "\n",
    "    # We will download a file containing the day list for an event\n",
    "    url = \"http://trecis.org/CrisisFACTs/CrisisFACTS-\"+eventNo+\".requests.json\"\n",
    "\n",
    "    # Download the list and parse as JSON\n",
    "    dayList = requests.get(url).json()\n",
    "\n",
    "    # Print each day\n",
    "    # Note each day object contains the following fields\n",
    "    #   {\n",
    "    #      \"eventID\" : \"CrisisFACTS-001\",\n",
    "    #      \"requestID\" : \"CrisisFACTS-001-r3\",\n",
    "    #      \"dateString\" : \"2017-12-07\",\n",
    "    #      \"startUnixTimestamp\" : 1512604800,\n",
    "    #      \"endUnixTimestamp\" : 1512691199\n",
    "    #   }\n",
    "\n",
    "    return dayList\n",
    "\n",
    "for day in getDaysForEventNo(eventNoList[9]):\n",
    "    print(day[\"dateString\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each day, we collected related content to the event from the following sources:\n",
    "\n",
    "\n",
    "*   **Twitter**: We are re-using tweets collected as part of the TREC Incident Streams track (http://trecis.org). These tweets were crawled by keyword, and as such most are likely to be relevant to the event, but are not nessessaraly good candidates for inclusion into a summary of what is happening.\n",
    "*   **Reddit**: Discussions regarding what happens during events also occurs on the forum platform Reddit. We collected relevant Reddit threads to each event, where we include both the original submission and subsequent comments within those threads.\n",
    "*   **News**: Traditional news agencies are often a good source of information during an emergency and so we have also included a small number of news articles collected during each event as well.\n",
    "\n",
    "Because these sources have different formatting and characteristics, we reformatted this data into a list of standardized 'stream items', where a stream item contains:\n",
    "\n",
    "\n",
    "*   **event**: The identifier of the event, e.g. 'CrisisFACTS-001'\n",
    "*   **streamID**: A unique identifier for the stream item. This will generally be of the form 'CrisisFACTS-\\<eventNo\\>-\\<source\\>-\\<postID\\>-\\<sentenceID\\>', e.g. CrisisFACTS-001-Twitter-15712-0.\n",
    "*   **unixTimestamp**: This is the time that the content was originally posted, expressed as a unix timestamp in seconds (UTC timezone).\n",
    "*   **text**: The text of the stream item. The maximum length of a stream item is 200 characters. \n",
    "*   **sourceType**: A string denoting the source, i.e. either Twitter, Reddit, News or Facebook.\n",
    "*   **source**: This is the original post content formated as JSON (ir_datasets ignores this field).\n",
    "\n",
    "Since, some types of content are longer than others (compare a news article vs. a tweet for instance), for long-form content we perform sentence segmentation, so one input post might form multiple stream items. In these cases, the 'sentenceID' component of the streamID denotes the number of the sentence in the source content.\n",
    "\n",
    "\n",
    "The dataset is structured by day and event. To access the stream items for a particular \\<event,day\\> pair we generate a request string specifying the day and event we want, of the form:\n",
    "\n",
    "*   '**crisisfacts/\\<eventNo\\>/\\<day\\>**'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavpatil/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched data for date: 2020-01-23 (11207 items)\n",
      "Successfully fetched data for date: 2020-01-24 (23981 items)\n",
      "Successfully fetched data for date: 2020-01-25 (5000 items)\n",
      "Successfully fetched data for date: 2020-01-26 (10698 items)\n",
      "Successfully fetched data for date: 2020-01-27 (10574 items)\n",
      "Successfully fetched data for date: 2020-01-28 (2931 items)\n",
      "Successfully fetched data for date: 2020-01-29 (709 items)\n",
      "                        doc_id            event  \\\n",
      "0  CrisisFACTS-010-Twitter-0-0  CrisisFACTS-010   \n",
      "1  CrisisFACTS-010-Twitter-0-1  CrisisFACTS-010   \n",
      "2  CrisisFACTS-010-Twitter-1-0  CrisisFACTS-010   \n",
      "3  CrisisFACTS-010-Twitter-2-0  CrisisFACTS-010   \n",
      "4  CrisisFACTS-010-Twitter-3-0  CrisisFACTS-010   \n",
      "\n",
      "                                           text  \\\n",
      "0                                  @TheNerdyEsq   \n",
      "1                            All the shout outs   \n",
      "2  Typo- was supposed to be good sportsmanship!   \n",
      "3                                           ???   \n",
      "4                       CEO of a market economy   \n",
      "\n",
      "                                              source source_type  \\\n",
      "0  {\"created_at\":\"Thu Jan 23 00:00:03 +0000 2020\"...     Twitter   \n",
      "1  {\"created_at\":\"Thu Jan 23 00:00:03 +0000 2020\"...     Twitter   \n",
      "2  {\"created_at\":\"Thu Jan 23 00:00:06 +0000 2020\"...     Twitter   \n",
      "3  {\"created_at\":\"Thu Jan 23 00:00:20 +0000 2020\"...     Twitter   \n",
      "4  {\"created_at\":\"Thu Jan 23 00:00:36 +0000 2020\"...     Twitter   \n",
      "\n",
      "   unix_timestamp  \n",
      "0      1579737603  \n",
      "1      1579737603  \n",
      "2      1579737606  \n",
      "3      1579737620  \n",
      "4      1579737636  \n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_crisisfacts_data(event_id, dates):\n",
    "    \"\"\"\n",
    "    Fetches data for a specified CrisisFACTS event and a list of dates.\n",
    "    \n",
    "    Parameters:\n",
    "        event_id (str): The CrisisFACTS event identifier (e.g., '001').\n",
    "        dates (list): A list of dates in 'YYYY-MM-DD' format for which to fetch the data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all the data fetched for the specified dates.\n",
    "    \"\"\"\n",
    "    all_data = []  # Initialize a list to store data for all dates\n",
    "\n",
    "    for date in dates:\n",
    "        try:\n",
    "            # Load the dataset for the given event and date\n",
    "            dataset = ir_datasets.load(f'crisisfacts/{event_id}/{date}')\n",
    "            \n",
    "            # Collect all documents from the iterator\n",
    "            data = list(dataset.docs_iter())\n",
    "            all_data.extend(data)  # Append data for this date to the main list\n",
    "            \n",
    "            print(f\"Successfully fetched data for date: {date} ({len(data)} items)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for date {date}: {e}\")\n",
    "    \n",
    "    # Convert the collected data to a Pandas DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "# List of dates for which to fetch data\n",
    "dates = [\n",
    "    \"2020-01-23\",\n",
    "    \"2020-01-24\",\n",
    "    \"2020-01-25\",\n",
    "    \"2020-01-26\",\n",
    "    \"2020-01-27\",\n",
    "    \"2020-01-28\",\n",
    "    \"2020-01-29\"\n",
    "]\n",
    "\n",
    "# Event identifier\n",
    "event_id = \"010\"\n",
    "\n",
    "# Fetch the data\n",
    "data_df = fetch_crisisfacts_data(event_id, dates)\n",
    "\n",
    "# Show a preview of the DataFrame\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        doc_id            event  \\\n",
      "0  CrisisFACTS-010-Twitter-0-0  CrisisFACTS-010   \n",
      "1  CrisisFACTS-010-Twitter-0-1  CrisisFACTS-010   \n",
      "2  CrisisFACTS-010-Twitter-1-0  CrisisFACTS-010   \n",
      "3  CrisisFACTS-010-Twitter-2-0  CrisisFACTS-010   \n",
      "4  CrisisFACTS-010-Twitter-3-0  CrisisFACTS-010   \n",
      "\n",
      "                                           text  \\\n",
      "0                                  @TheNerdyEsq   \n",
      "1                            All the shout outs   \n",
      "2  Typo- was supposed to be good sportsmanship!   \n",
      "3                                           ???   \n",
      "4                       CEO of a market economy   \n",
      "\n",
      "                                              source source_type  \\\n",
      "0  {\"created_at\":\"Thu Jan 23 00:00:03 +0000 2020\"...     Twitter   \n",
      "1  {\"created_at\":\"Thu Jan 23 00:00:03 +0000 2020\"...     Twitter   \n",
      "2  {\"created_at\":\"Thu Jan 23 00:00:06 +0000 2020\"...     Twitter   \n",
      "3  {\"created_at\":\"Thu Jan 23 00:00:20 +0000 2020\"...     Twitter   \n",
      "4  {\"created_at\":\"Thu Jan 23 00:00:36 +0000 2020\"...     Twitter   \n",
      "\n",
      "   unix_timestamp  \n",
      "0      1579737603  \n",
      "1      1579737603  \n",
      "2      1579737606  \n",
      "3      1579737620  \n",
      "4      1579737636  \n"
     ]
    }
   ],
   "source": [
    "twitter_data = data_df[data_df['source_type'] == \"Twitter\"]\n",
    "print(twitter_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          doc_id            event  \\\n",
      "1753  CrisisFACTS-010-Reddit-0-0  CrisisFACTS-010   \n",
      "1754  CrisisFACTS-010-Reddit-0-1  CrisisFACTS-010   \n",
      "1755  CrisisFACTS-010-Reddit-0-2  CrisisFACTS-010   \n",
      "1756  CrisisFACTS-010-Reddit-0-3  CrisisFACTS-010   \n",
      "1757  CrisisFACTS-010-Reddit-0-4  CrisisFACTS-010   \n",
      "\n",
      "                                                   text  \\\n",
      "1753  The interesting bits from \"Siege of Terra : So...   \n",
      "1754                    This is not a detailed summary.   \n",
      "1755  It's a collection of interesting stuff and plo...   \n",
      "1756  With that said, lets go.\\n\\n&#x200B;\\n\\n&#x200...   \n",
      "1757  The new master of Terra doesn't have the wisdo...   \n",
      "\n",
      "                                                 source source_type  \\\n",
      "1753  {\"subreddit_display\": \"Fun is over, get back t...      Reddit   \n",
      "1754  {\"subreddit_display\": \"Fun is over, get back t...      Reddit   \n",
      "1755  {\"subreddit_display\": \"Fun is over, get back t...      Reddit   \n",
      "1756  {\"subreddit_display\": \"Fun is over, get back t...      Reddit   \n",
      "1757  {\"subreddit_display\": \"Fun is over, get back t...      Reddit   \n",
      "\n",
      "      unix_timestamp  \n",
      "1753      1579747636  \n",
      "1754      1579747636  \n",
      "1755      1579747636  \n",
      "1756      1579747636  \n",
      "1757      1579747636  \n"
     ]
    }
   ],
   "source": [
    "reddit_data = data_df[data_df['source_type'] == \"Reddit\"]\n",
    "print(reddit_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       doc_id            event  \\\n",
      "314  CrisisFACTS-010-News-0-0  CrisisFACTS-010   \n",
      "315  CrisisFACTS-010-News-0-1  CrisisFACTS-010   \n",
      "316  CrisisFACTS-010-News-0-2  CrisisFACTS-010   \n",
      "317  CrisisFACTS-010-News-0-3  CrisisFACTS-010   \n",
      "318  CrisisFACTS-010-News-0-4  CrisisFACTS-010   \n",
      "\n",
      "                                                  text  \\\n",
      "314  The Global Trust Crisis: As the world’s politi...   \n",
      "315  In recent months, protests have multiplied acr...   \n",
      "316  Unlike previous waves of protests such as the ...   \n",
      "317  Unless the leaders convening at Davos can form...   \n",
      "318           Why are so many people so disillusioned?   \n",
      "\n",
      "                                                source source_type  \\\n",
      "314  {\"id\": \"foreignpolicy--2020-01-22--The Global ...        News   \n",
      "315  {\"id\": \"foreignpolicy--2020-01-22--The Global ...        News   \n",
      "316  {\"id\": \"foreignpolicy--2020-01-22--The Global ...        News   \n",
      "317  {\"id\": \"foreignpolicy--2020-01-22--The Global ...        News   \n",
      "318  {\"id\": \"foreignpolicy--2020-01-22--The Global ...        News   \n",
      "\n",
      "     unix_timestamp  \n",
      "314      1579739089  \n",
      "315      1579739089  \n",
      "316      1579739089  \n",
      "317      1579739089  \n",
      "318      1579739089  \n"
     ]
    }
   ],
   "source": [
    "news_data = data_df[data_df['source_type'] == \"News\"]\n",
    "print(news_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and cleaning\n",
    "Using Spark for scalability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 10:43:32 WARN Utils: Your hostname, Pranavs-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.0.0.14 instead (on interface en0)\n",
      "24/12/08 10:43:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/08 10:43:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/08 10:43:39 WARN TaskSetManager: Stage 0 contains a task of very large size (27478 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/08 10:43:44 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-----------+--------------+\n",
      "|              doc_id|          event|                text|              source|source_type|unix_timestamp|\n",
      "+--------------------+---------------+--------------------+--------------------+-----------+--------------+\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|typo was supposed...|{\"created_at\":\"Th...|    Twitter|    1579737606|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|ceo of a market e...|{\"created_at\":\"Th...|    Twitter|    1579737636|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|stopping the cloc...|{\"created_at\":\"Th...|    Twitter|    1579737656|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|do you know someo...|{\"created_at\":\"Th...|    Twitter|    1579737671|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|realestate httpst...|{\"created_at\":\"Th...|    Twitter|    1579737671|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|do you know someo...|{\"created_at\":\"Th...|    Twitter|    1579737675|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|realestate httpst...|{\"created_at\":\"Th...|    Twitter|    1579737675|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|check out our new...|{\"created_at\":\"Th...|    Twitter|    1579737677|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|tell us what you ...|{\"created_at\":\"Th...|    Twitter|    1579737677|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|realestate httpst...|{\"created_at\":\"Th...|    Twitter|    1579737677|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|thenerdyesq https...|{\"created_at\":\"Th...|    Twitter|    1579737677|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|kihublue try out ...|{\"created_at\":\"Th...|    Twitter|    1579737681|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|repost rbrw with ...|{\"created_at\":\"Th...|    Twitter|    1579737686|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|its launch party ...|{\"created_at\":\"Th...|    Twitter|    1579737686|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|join the real b h...|{\"created_at\":\"Th...|    Twitter|    1579737686|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|the neglect is re...|{\"created_at\":\"Th...|    Twitter|    1579737695|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|thejayburns it be...|{\"created_at\":\"Th...|    Twitter|    1579737700|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|you just gotta ro...|{\"created_at\":\"Th...|    Twitter|    1579737700|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|lmaokar httpstco3...|{\"created_at\":\"Th...|    Twitter|    1579737716|\n",
      "|CrisisFACTS-010-T...|CrisisFACTS-010|got 200 for a sho...|{\"created_at\":\"Th...|    Twitter|    1579737721|\n",
      "+--------------------+---------------+--------------------+--------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 10:43:45 WARN TaskSetManager: Stage 1 contains a task of very large size (27478 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/08 10:43:46 WARN MemoryManager: Total allocation exceeds 95.00% (926,914,958 bytes) of heap memory\n",
      "Scaling row group sizes to 98.66% for 7 writers\n",
      "24/12/08 10:43:46 WARN MemoryManager: Total allocation exceeds 95.00% (926,914,958 bytes) of heap memory\n",
      "Scaling row group sizes to 86.33% for 8 writers\n",
      "24/12/08 10:43:55 WARN MemoryManager: Total allocation exceeds 95.00% (926,914,958 bytes) of heap memory\n",
      "Scaling row group sizes to 98.66% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, lower, regexp_replace\n",
    "\n",
    "def preprocess_data(dataframes, spark):\n",
    "    \"\"\"\n",
    "    Preprocess multiple datasets by cleaning text data.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframes (list): List of Spark DataFrames to be cleaned.\n",
    "        spark (SparkSession): An active Spark session.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A single cleaned DataFrame combining all datasets.\n",
    "    \"\"\"\n",
    "    combined_df = None  # Initialize variable to store combined DataFrame\n",
    "\n",
    "    for df in dataframes:\n",
    "        # Clean the text data\n",
    "        cleaned = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\s]\", \"\")) \\\n",
    "                    .withColumn(\"text\", lower(col(\"text\"))) \\\n",
    "                    .filter(length(col(\"text\")) > 20)\n",
    "        \n",
    "        # Combine with the main DataFrame\n",
    "        if combined_df is None:\n",
    "            combined_df = cleaned\n",
    "        else:\n",
    "            combined_df = combined_df.union(cleaned)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CRISISFacts Preprocessing\").getOrCreate()\n",
    "\n",
    "# Create Spark DataFrames for Twitter, Reddit, and News data\n",
    "twitter_df = spark.createDataFrame(twitter_data)  \n",
    "reddit_df = spark.createDataFrame(reddit_data) \n",
    "news_df = spark.createDataFrame(news_data) \n",
    "\n",
    "# Combine all datasets into a list\n",
    "all_dataframes = [twitter_df, reddit_df, news_df]\n",
    "\n",
    "# Preprocess all datasets\n",
    "cleaned_df = preprocess_data(all_dataframes, spark)\n",
    "\n",
    "# Show the first few rows of the cleaned data\n",
    "cleaned_df.show()\n",
    "\n",
    "# Save the cleaned DataFrame for further processing\n",
    "cleaned_df.write.mode(\"overwrite\").parquet(\"cleaned_crisisfacts_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 10:46:04 WARN TaskSetManager: Stage 2 contains a task of very large size (27478 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the cleaned data as CSV for compatibility with other tools\n",
    "cleaned_df.write.mode(\"overwrite\").csv(\"cleaned_crisisfacts_data.csv\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
